{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7a9a9d",
   "metadata": {},
   "source": [
    "# Enhanced Flower Classification - Version 2.0\n",
    "\n",
    "## ðŸŽ¯ Key Improvements:\n",
    "- **Advanced Data Augmentation** - More diverse transformations\n",
    "- **Class Imbalance Handling** - Weighted training for balanced learning\n",
    "- **Enhanced Visualizations** - Better dataset insights\n",
    "- **Improved Models** - Optimized architectures with regularization\n",
    "\n",
    "### Version 1 Results (Baseline):\n",
    "- Simple CNN: 54% (overfitting)\n",
    "- VGG16: 84% \n",
    "- MobileNetV2: 88% (best)\n",
    "\n",
    "**Goal**: Achieve >90% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16, MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(\"ðŸš€ Enhanced System Ready\")\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Configuration\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.0001\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "TRAIN_DIR = \"archive_2/train\"\n",
    "TEST_DIR = \"archive_2/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a85b7",
   "metadata": {},
   "source": [
    "## ðŸ“Š Enhanced Dataset Analysis with Class Imbalance Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution and detect imbalance\n",
    "def analyze_dataset(data_dir):\n",
    "    class_counts = {}\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            count = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            class_counts[class_name] = count\n",
    "    \n",
    "    counts = list(class_counts.values())\n",
    "    imbalance_ratio = max(counts) / min(counts) if min(counts) > 0 else 0\n",
    "    \n",
    "    print(f\"Total: {sum(counts):,} | Classes: {len(class_counts)} | Imbalance: {imbalance_ratio:.2f}\")\n",
    "    if imbalance_ratio > 1.5:\n",
    "        print(\"âš ï¸ Imbalance detected - applying class weights\")\n",
    "    \n",
    "    return class_counts, imbalance_ratio\n",
    "\n",
    "train_classes, train_imbalance = analyze_dataset(TRAIN_DIR)\n",
    "test_classes, _ = analyze_dataset(TEST_DIR)\n",
    "class_names = list(train_classes.keys())\n",
    "\n",
    "# Enhanced visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle('ðŸŽ¯ Enhanced Dataset Analysis', fontweight='bold')\n",
    "\n",
    "# Class distribution\n",
    "train_counts = [train_classes[name] for name in class_names]\n",
    "test_counts = [test_classes[name] for name in class_names]\n",
    "x = range(len(class_names))\n",
    "\n",
    "axes[0].bar([i-0.2 for i in x], train_counts, 0.4, label='Train', alpha=0.8)\n",
    "axes[0].bar([i+0.2 for i in x], test_counts, 0.4, label='Test', alpha=0.8)\n",
    "axes[0].set_title('Class Distribution')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(class_names, rotation=45)\n",
    "axes[0].legend()\n",
    "\n",
    "# Imbalance visualization\n",
    "max_count = max(train_counts)\n",
    "imbalance_factors = [max_count/count for count in train_counts]\n",
    "axes[1].bar(class_names, imbalance_factors, color=sns.color_palette(\"viridis\", len(class_names)))\n",
    "axes[1].set_title('Class Imbalance Factors')\n",
    "axes[1].set_xticklabels(class_names, rotation=45)\n",
    "axes[1].axhline(y=1.5, color='red', linestyle='--', alpha=0.7, label='Threshold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e5ea0",
   "metadata": {},
   "source": [
    "## âš–ï¸ Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for balanced training\n",
    "def calculate_class_weights(class_counts):\n",
    "    classes = list(class_counts.keys())\n",
    "    counts = np.array(list(class_counts.values()))\n",
    "    \n",
    "    classes_sklearn = np.arange(len(classes))\n",
    "    sample_weights = np.repeat(classes_sklearn, counts)\n",
    "    weights = compute_class_weight('balanced', classes=classes_sklearn, y=sample_weights)\n",
    "    \n",
    "    class_weights_dict = {i: weights[i] for i in range(len(classes))}\n",
    "    \n",
    "    print(\"âš–ï¸ Class Weights:\")\n",
    "    for i, class_name in enumerate(classes):\n",
    "        print(f\"{class_name}: {weights[i]:.3f}\")\n",
    "    \n",
    "    return class_weights_dict\n",
    "\n",
    "class_weights = calculate_class_weights(train_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf766a9",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Advanced Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44994096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced data generators with advanced augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    brightness_range=[0.6, 1.4],\n",
    "    channel_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=VALIDATION_SPLIT)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create datasets\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', subset='training', shuffle=True, seed=42\n",
    ")\n",
    "\n",
    "val_ds = val_datagen.flow_from_directory(\n",
    "    TRAIN_DIR, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', subset='validation', shuffle=False, seed=42\n",
    ")\n",
    "\n",
    "test_ds = test_datagen.flow_from_directory(\n",
    "    TEST_DIR, target_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"ðŸ”„ Advanced augmentation applied\")\n",
    "print(f\"Train: {train_ds.samples} | Val: {val_ds.samples} | Test: {test_ds.samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d480bc",
   "metadata": {},
   "source": [
    "# ðŸ—ï¸ Enhanced Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced CNN with improved regularization\n",
    "def create_enhanced_cnn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        \n",
    "        # Block 1 with BatchNorm and Dropout\n",
    "        layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(2),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 2\n",
    "        layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D(2),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(2),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Classifier with GlobalAveragePooling\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Enhanced VGG16\n",
    "def create_enhanced_vgg16():\n",
    "    base = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    base.trainable = False\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        base,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Enhanced MobileNetV2\n",
    "def create_enhanced_mobilenet():\n",
    "    base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    base.trainable = False\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        base,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile models\n",
    "enhanced_cnn = create_enhanced_cnn()\n",
    "enhanced_vgg16 = create_enhanced_vgg16()\n",
    "enhanced_mobilenet = create_enhanced_mobilenet()\n",
    "\n",
    "for model in [enhanced_cnn, enhanced_vgg16, enhanced_mobilenet]:\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "print(\"ðŸ—ï¸ Enhanced models created with improved architectures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f80eae",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Training with Enhanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training with callbacks and class weights\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "]\n",
    "\n",
    "print(\"ðŸš€ Training Enhanced Models with Class Weights...\")\n",
    "\n",
    "# Train Enhanced CNN\n",
    "print(\"\\n1. Training Enhanced CNN...\")\n",
    "history_cnn = enhanced_cnn.fit(\n",
    "    train_ds, epochs=EPOCHS, validation_data=val_ds,\n",
    "    class_weight=class_weights, callbacks=callbacks, verbose=1\n",
    ")\n",
    "\n",
    "# Train Enhanced VGG16\n",
    "print(\"\\n2. Training Enhanced VGG16...\")\n",
    "history_vgg = enhanced_vgg16.fit(\n",
    "    train_ds, epochs=EPOCHS, validation_data=val_ds,\n",
    "    class_weight=class_weights, callbacks=callbacks, verbose=1\n",
    ")\n",
    "\n",
    "# Train Enhanced MobileNetV2\n",
    "print(\"\\n3. Training Enhanced MobileNetV2...\")\n",
    "history_mobile = enhanced_mobilenet.fit(\n",
    "    train_ds, epochs=EPOCHS, validation_data=val_ds,\n",
    "    class_weight=class_weights, callbacks=callbacks, verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… All enhanced models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77669f9",
   "metadata": {},
   "source": [
    "# ðŸ“Š Enhanced Results & Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('ðŸŽ¯ Enhanced Models Performance', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Validation accuracy comparison\n",
    "axes[0].plot(history_cnn.history['val_accuracy'], label='Enhanced CNN', linewidth=2)\n",
    "axes[0].plot(history_vgg.history['val_accuracy'], label='Enhanced VGG16', linewidth=2)\n",
    "axes[0].plot(history_mobile.history['val_accuracy'], label='Enhanced MobileNet', linewidth=2)\n",
    "axes[0].set_title('Validation Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss comparison\n",
    "axes[1].plot(history_cnn.history['val_loss'], label='Enhanced CNN', linewidth=2)\n",
    "axes[1].plot(history_vgg.history['val_loss'], label='Enhanced VGG16', linewidth=2)\n",
    "axes[1].plot(history_mobile.history['val_loss'], label='Enhanced MobileNet', linewidth=2)\n",
    "axes[1].set_title('Validation Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test evaluation and comparison\n",
    "def evaluate_model(model, model_name):\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "    print(f\"{model_name}: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    return test_acc\n",
    "\n",
    "print(\"ðŸ† FINAL TEST RESULTS:\")\n",
    "print(\"=\" * 40)\n",
    "cnn_acc = evaluate_model(enhanced_cnn, \"Enhanced CNN      \")\n",
    "vgg_acc = evaluate_model(enhanced_vgg16, \"Enhanced VGG16    \")\n",
    "mobile_acc = evaluate_model(enhanced_mobilenet, \"Enhanced MobileNet\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ IMPROVEMENT ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Model\\t\\t\\tOriginal\\tEnhanced\\tGain\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"CNN\\t\\t\\t54.0%\\t\\t{cnn_acc*100:.1f}%\\t\\t+{(cnn_acc*100-54.0):.1f}%\")\n",
    "print(f\"VGG16\\t\\t\\t84.0%\\t\\t{vgg_acc*100:.1f}%\\t\\t+{(vgg_acc*100-84.0):.1f}%\")\n",
    "print(f\"MobileNetV2\\t\\t88.0%\\t\\t{mobile_acc*100:.1f}%\\t\\t+{(mobile_acc*100-88.0):.1f}%\")\n",
    "\n",
    "best_acc = max(cnn_acc, vgg_acc, mobile_acc)\n",
    "best_model = [\"Enhanced CNN\", \"Enhanced VGG16\", \"Enhanced MobileNet\"][[cnn_acc, vgg_acc, mobile_acc].index(best_acc)]\n",
    "\n",
    "print(f\"\\nðŸ¥‡ Best Model: {best_model} with {best_acc*100:.2f}% accuracy\")\n",
    "print(f\"ðŸŽ¯ Goal Achievement: {'âœ… SUCCESS' if best_acc > 0.9 else 'âŒ CLOSE'} (Target: >90%)\")\n",
    "\n",
    "# Key improvements summary\n",
    "print(\"\\nðŸš€ KEY ENHANCEMENTS APPLIED:\")\n",
    "print(\"âœ… Advanced data augmentation (8 techniques)\")\n",
    "print(\"âœ… Class imbalance handling with weighted training\")\n",
    "print(\"âœ… Improved model architectures with regularization\")\n",
    "print(\"âœ… Enhanced training with callbacks\")\n",
    "print(\"âœ… Better data visualization and analysis\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
